{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before we start developing with Elasticsearch, let's make sure \n",
    "\n",
    "- we can connect to the cluster,\n",
    "- some data are already available,\n",
    "- and we are able to perform a query on the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Test Connection\n",
    "client = Elasticsearch(hosts=\"http://localhost:9200\",\n",
    "                       basic_auth=('elastic', 'changeme'))\n",
    "client.indices.put_settings(\n",
    "    settings={\n",
    "        \"index.number_of_replicas\": 0\n",
    "    })\n",
    "\n",
    "assert client.cluster.health()['status'] == 'green', 'Cluster not healthy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection should be established and the cluster health green - which means we're good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index\n",
    "\n",
    "response = client.indices.create(index='my-first-index')\n",
    "\n",
    "assert response['acknowledged'], 'Index could not be created'\n",
    "assert client.indices.exists(index='my-first-index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a sample document\n",
    "\n",
    "client.index(index='my-first-index',id=23,document={\n",
    "    'summary': 'Michael Jeffrey Jordan (born February 17, 1963), also known by his initials MJ, is an American businessman and former professional basketball player. He played fifteen seasons in the National Basketball Association (NBA), winning six NBA championships with the Chicago Bulls. Jordan is the principal owner and chairman of the Charlotte Hornets of the NBA and of 23XI Racing in the NASCAR Cup Series. His biography on the official NBA website states: \"By acclamation, Michael Jordan is the greatest basketball player of all time.\" He was integral in popularizing the NBA around the world in the 1980s and 1990s, becoming a global cultural icon in the process.'\n",
    "})\n",
    "assert client.exists(index='my-first-index',id=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've created an index and indexed one document - not bad, but we're just getting started! Let's try to search our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be easy\n",
    "\n",
    "client.search(index='my-first-index', query={\n",
    "    'match': {\n",
    "        'summary': 'Michael'\n",
    "    }},\n",
    "    highlight={'fields': {'summary': {}}}\n",
    ").body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe Elasticsearch has even more answers?\n",
    "\n",
    "client.search(query={\n",
    "    'match': {\n",
    "        'summary': 'GOAT'\n",
    "    }},\n",
    "    highlight={'fields': {'summary': {}}}\n",
    ").body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing the set\n",
    "\n",
    "We can already explore the search API with this minimal example, however, to make things more interesting, we should probably accumulate a bit more data. While setting up the cluster, some articles from Wikipedia were already ingested into the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the indexed documents\n",
    "\n",
    "client.count(index='logstash-articles').body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the prepared index, around a thousand documents are stored - that's more than one, and most are a bit longer than one paragraph.\n",
    "\n",
    "When dealing with new data, it's usually a good idea to get a feel for the format of the data. Feel free to use the launched Kibana app to explore the dataset! (Running on port 5601.) Here, we're content with a quick look at the fields containing data:\n",
    "\n",
    "- content\n",
    "- links\n",
    "- summary\n",
    "- tags\n",
    "- title\n",
    "- url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the mapping\n",
    "\n",
    "client.indices.get_mapping(index='logstash-articles').body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how many results we get for the same query as above ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michael appears to be a fairly common name!\n",
    "\n",
    "client.search(index='logstash-articles', query={\n",
    "    'match': {\n",
    "        'summary': 'michael'\n",
    "    }}, \n",
    "    highlight={'fields': {'summary': {}}},\n",
    "    source=['summary'],\n",
    ").body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "Elasticsearch indexes data using an [inverted index](https://en.wikipedia.org/wiki/Inverted_index), allowing fast searches across massive textual datasets. By default, texts / strings are indexed as type [`text`](https://www.elastic.co/guide/en/elasticsearch/reference/current/text.html) as well as [`keyword`](https://www.elastic.co/guide/en/elasticsearch/reference/current/keyword.html). So far, we have been searching on fields of type `text`, using the default [standard analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html), that tokenizes text and works well in most generic use cases. However, it has limitations once typos, a google-like search-as-you-type feel or domain-specific normalization come into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fairly standard typo yields no results\n",
    "\n",
    "client.search(index='logstash-articles', query={\n",
    "    'match': {\n",
    "        'summary': 'micheal'\n",
    "    }},\n",
    "    highlight={'fields': {'summary': {}}},\n",
    "    source=['summary'],\n",
    ").body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical features\n",
    "\n",
    "To address such use cases, Elasticsearch offers an array of instruments that can be used individually or in conjunction to match feature requests and improve the accuraccy of search results. We will explore a couple of features next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams\n",
    "\n",
    "Ngrams are a well-known concept in linguistics and Natural Language Processing (NLP). The idea is to split single words into a number of letter tokens with a maximum defined length, e.g. 3. A word, such as _length_ would then be split into tokens with a maximum length of three.\n",
    "\n",
    "Let's examine the ngrams yielded by [the default tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-ngram-tokenizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t['token'] for t in client.indices.analyze(\n",
    "    text='lengthy',\n",
    "    tokenizer='ngram'\n",
    ").body['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the word `lengthy` is split into tokens with a minimum size of 1 and a maximum size of 2. Let's add a field to our index that uses ngrams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to define an analyzer that uses the ngram tokenizer\n",
    "assert client.indices.close(index='logstash-articles').body['acknowledged'], 'Technical error' # technically, we need to close the index temporarily - don't do this on a live system unless you know no new data will be added while index is closed\n",
    "\n",
    "client.indices.put_settings(\n",
    "    index='logstash-articles',\n",
    "    settings={\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'ngram_analyzer': {\n",
    "                    'tokenizer': 'ngram_tokenizer'\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'ngram_tokenizer': {\n",
    "                    'type': 'ngram',\n",
    "                    'min_gram': 2,\n",
    "                    'max_gram': 3,\n",
    "                    'token_chars': [\n",
    "                        'letter',\n",
    "                        'digit'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "assert client.indices.open(index='logstash-articles').body['acknowledged'], 'Technical error' # reopen index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the analyzer\n",
    "[t['token'] for t in client.indices.analyze(\n",
    "    index='logstash-articles',\n",
    "    text='lengthy',\n",
    "    analyzer='ngram_analyzer'\n",
    ").body['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the tokenizer\n",
    "[t['token'] for t in client.indices.analyze(\n",
    "    index='logstash-articles',\n",
    "    text='lengthy',\n",
    "    tokenizer='ngram_tokenizer'\n",
    ").body['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and define a field that uses the analyzer\n",
    "\n",
    "assert client.indices.put_mapping(\n",
    "    index='logstash-articles',\n",
    "    properties={\n",
    "        'summary': {\n",
    "            'type': 'text',\n",
    "            'norms': 'false',\n",
    "            'fields': {\n",
    "                'ngram': {\n",
    "                    'type': 'text',\n",
    "                    'analyzer': 'ngram_analyzer'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ").body['acknowledged'], 'Technical error'\n",
    "\n",
    "assert client.update_by_query(index='logstash-articles').body['updated'] == 1000, 'Technical error' # refresh index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we should be able to find something despite the type\n",
    "\n",
    "client.search(index='logstash-articles', query={\n",
    "    'match': {\n",
    "        'summary.ngram': 'micheal'\n",
    "    }},\n",
    "    highlight={'fields': {'summary.ngram': {}}},\n",
    "    source=['summary'],\n",
    ").body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, results the results are certainly not yet what we want. Remember, the `ngram` tokenizer does nothing different than split words into tokens, so we should not be surprised that results appear a bit random. After all, let's have a look at the ngrams derived from _Micheal_:\n",
    "\n",
    "    ['mi', 'mic', 'ic', 'ich', 'ch', 'che', 'he', 'hea', 'ea', 'eal', 'al']\n",
    "\n",
    "Though some ngrams would also appear in the tokenization of _Michael_, there is nothing special about them - Elasticsearch will simply match with the documents that contain the tokens most often. In general, it takes a little testing and trial-and-error to find the best minimum and maximum values for ngram length (the longer, the more specific the matches, but less error-tolerant). Let's dig deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search-as-you-type\n",
    "\n",
    "The [search-as-you-type functionality](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-as-you-type.html) utilizes ngrams in a slighty different way compared to what we have done so far. Instead of looking at the entire word for tokenization, the focus is on the beginning of the words to offer a fast search functionality that offers results as a user is typing the first few letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert client.indices.put_mapping(\n",
    "    index='logstash-articles',\n",
    "    properties={\n",
    "        'summary': {\n",
    "            'type': 'text',\n",
    "            'norms': 'false',\n",
    "            'fields': {\n",
    "                'search_as_you_type': {\n",
    "                    'type': 'search_as_you_type'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ").body['acknowledged'], 'Technical error'\n",
    "\n",
    "assert client.update_by_query(index='logstash-articles').body['updated'] == 1000, 'Technical error' # refresh index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can actually query on multiple fields that are generated automatically for us\n",
    "\n",
    "client.search(index='logstash-articles', query={\n",
    "    \"multi_match\": {\n",
    "        \"query\": \"mi\",\n",
    "        \"type\": \"bool_prefix\",\n",
    "        \"fields\": [\n",
    "            \"summary.search_as_you_type\",\n",
    "            \"summary.search_as_you_type._2gram\",\n",
    "            \"summary.search_as_you_type._3gram\",\n",
    "            \"summary.search_as_you_type._index_prefix\"\n",
    "        ]\n",
    "    }},\n",
    "    source=['summary'],\n",
    ").body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonetic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further down the rabbit hole"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
